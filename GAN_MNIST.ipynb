{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import matplotlib.gridspec as gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, reuse=False):\n",
    "    with tf.variable_scope('generator',reuse=reuse):\n",
    "        with tf.variable_scope('dense1',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[100,256],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[256],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense1 = tf.nn.relu(tf.matmul(z,weight) + bias)\n",
    "        \n",
    "        with tf.variable_scope('dense2',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[256,512],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[512],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense2 = tf.nn.relu(tf.matmul(dense1, weight)+bias)\n",
    "        \n",
    "        with tf.variable_scope('dense3',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[512,1024],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[1024],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense3 = tf.nn.relu(tf.matmul(dense2, weight) + bias)\n",
    "        \n",
    "        with tf.variable_scope('output',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[1024,784],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[784],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            output = tf.nn.tanh(tf.matmul(dense3, weight) + bias)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image, drop_out, reuse=False):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        with tf.variable_scope('dense1',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[784,1024],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[1024],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense1 = tf.nn.relu(tf.matmul(image,weight) + bias)\n",
    "            dense1 =tf.nn.dropout(dense1,drop_out)\n",
    "        \n",
    "        with tf.variable_scope('dense2',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[1024,512],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[512],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense2 = tf.nn.relu(tf.matmul(dense1,weight) + bias)\n",
    "            dense2 =tf.nn.dropout(dense2,drop_out)\n",
    "            \n",
    "        with tf.variable_scope('dense3',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[512,256],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[256],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            dense3 = tf.nn.relu(tf.matmul(dense2,weight) + bias)\n",
    "            dense3 =tf.nn.dropout(dense3,drop_out)\n",
    "        \n",
    "        with tf.variable_scope('output',reuse=reuse):\n",
    "            weight = tf.get_variable('weight',[256,1],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean=0.,stddev=0.02))\n",
    "            bias = tf.get_variable('bias',[1],initializer=tf.constant_initializer(0.))\n",
    "            \n",
    "            output = tf.nn.sigmoid(tf.matmul(dense3,weight) + bias)\n",
    "        \n",
    "        return output\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image\n",
    "def save_f_image(index, z_sample):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(z_sample):  # [i,samples[i]] imax=16\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='gray')\n",
    "    plt.savefig('{}.png'.format(str(index).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None,784))\n",
    "z = tf.placeholder(tf.float32, shape=(None,100))\n",
    "drop_out = tf.placeholder(tf.float32)\n",
    "\n",
    "f_image = generator(z)\n",
    "d_real = discriminator(x,drop_out)\n",
    "\n",
    "d_fake = discriminator(f_image,drop_out,reuse=True)\n",
    "'''\n",
    "d_loss = -tf.reduce_mean(tf.log(d_real) + tf.log(1. - d_fake))\n",
    "g_loss = -tf.reduce_mean(tf.log(d_fake))\n",
    "'''\n",
    "eps = 1e-2\n",
    "d_loss = tf.reduce_mean(-tf.log(d_real + eps) - tf.log(1. - d_fake + eps))\n",
    "g_loss = tf.reduce_mean(-tf.log(d_fake + eps))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "\n",
    "train_set = (mnist.train.images - 0.5)/0.5\n",
    "test_z = np.random.normal(0,1,(16,100))\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "print(d_vars)\n",
    "print(g_vars)\n",
    "d_train = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "index =0\n",
    "while epoch <= num_epochs:\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        fake_image = sess.run(f_image,feed_dict={z:test_z,drop_out:0.})\n",
    "        save_f_image(index, fake_image)\n",
    "        index += 1\n",
    "    for i in range(0,train_set.shape[0],batch_size):\n",
    "        sess.graph.finalize()\n",
    "        input_x = train_set[i:i+batch_size]\n",
    "        input_z = np.random.normal(0,1,(input_x.shape[0],100))\n",
    "        \n",
    "        sess.run(d_train, feed_dict={x:input_x, z:input_z,drop_out:0.3})\n",
    "        loss_d = sess.run(d_loss,feed_dict={x:input_x,z:input_z,drop_out:0.3})\n",
    "        d_losses.append(loss_d)\n",
    "        \n",
    "        input_z = np.random.normal(0,1,(input_x.shape[0],100))\n",
    "        \n",
    "        sess.run(g_train,feed_dict={z:input_z,drop_out:0.3})\n",
    "        loss_g = sess.run(g_loss,feed_dict={z:input_z,drop_out:0.3})\n",
    "        g_losses.append(loss_g)\n",
    "    print(\"@epoch of {}, D_loss: {}, G_loss: {}\".format(epoch+1,np.mean(d_losses),np.mean(g_losses)))\n",
    "    epoch += 1\n",
    "\n",
    "save_path = saver.save(sess, './gan_model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
